{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Pour le dataframe\n",
    "from skimage.transform import resize\n",
    "import numpy as np # Pour la normalisation et calculs de moyenne\n",
    "import matplotlib.pyplot as plt # Pour la visualisation\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import librosa # Pour l'extraction des features et la lecture des fichiers wav\n",
    "import librosa.display # Pour récupérer les spectrogrammes des audio\n",
    "import librosa.feature\n",
    "\n",
    "import os # C'est ce qui va nous permettre d'itérer sur les fichiers de l'environnement de travail\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve, RandomizedSearchCV # Split de dataset et optimisation des hyperparamètres\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # XGBoost\n",
    "from sklearn.neighbors import KNeighborsClassifier # k-NN\n",
    "from sklearn.svm import SVC # SVM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score, zero_one_loss, classification_report # Métriques pour la mesure de performances\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "\n",
    "import tensorflow as tf # Pour le reseau de neurones simple et pour le CNN\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voici la liste des genres musicaux représentés dans la notre base de données, on en compte 10.\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#On commence par initialiser le dictionnaire avec les clés qui sont les noms des genres.\n",
    "#On met autant de listes vides qu'il n'y a de genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire vide pour stocker les données audio pour chaque genre\n",
    "audio_files = {}\n",
    "\n",
    "# Itérer sur la liste des genres\n",
    "for g in genres:\n",
    "    # Créer une liste vide pour stocker les données audio pour le genre actuel\n",
    "    audio_files[g] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itérer sur les fichiers audio dans le répertoire pour le genre actuel\n",
    "for audio in os.listdir(f'./Data/genres_original/{g}'):\n",
    "    # Charger les données audio en utilisant librosa et les ajouter à la liste pour le genre actuel\n",
    "    audio_data = librosa.load(f'./Data/genres_original/{g}/{audio}')[0]\n",
    "    audio_files[g].append(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio) #Une chaîne de caractère est passée et non un échantillon audio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons présentés toutes les features que l'on a considéré, construisons un pipeline pour créer une base d'apprentissage.\\\n",
    "Toutes ces features sont facilement calculables avec la librairie Python Librosa.\\\n",
    "_Ce pipeline nous servira aussi pour essayer nos modèles sur des fichiers musicaux externes (YouTube par exemple)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_pipeline(audio):\n",
    "  \n",
    "  features = []\n",
    "\n",
    "  # Calcul du ZCR\n",
    "\n",
    "  zcr = librosa.zero_crossings(audio)\n",
    "  features.append(sum(zcr))\n",
    "\n",
    "  # Calcul de la moyenne du Spectral centroid\n",
    "\n",
    "  spectral_centroids = librosa.feature.spectral_centroid(y=audio)[0]\n",
    "  features.append(np.mean(spectral_centroids))\n",
    "  \n",
    "  # Calcul du spectral rolloff point\n",
    "\n",
    "  rolloff = librosa.feature.spectral_rolloff(y=audio)\n",
    "  features.append(np.mean(rolloff))\n",
    "\n",
    "  # Calcul des moyennes des MFCC\n",
    "\n",
    "  mfcc = librosa.feature.mfcc(y=audio)\n",
    "\n",
    "  for x in mfcc:\n",
    "    features.append(np.mean(x))\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On écrit les noms des features dans les colonnes d'un dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['zcr', 'spectral_c', 'rolloff', 'mfcc1', 'mfcc2', 'mfcc3',\n",
    "                'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9',\n",
    "                'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15',\n",
    "                'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for g in genres:\n",
    "  for music in audio_files[g]:\n",
    "    df.loc[i] = audio_pipeline(music)+[g]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./csv_result/result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./csv_result/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=(0.2))\n",
    "selected_features = selector.fit_transform(df[['zcr', 'spectral_c', 'rolloff', 'mfcc1', 'mfcc2', 'mfcc3',\n",
    "                                              'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9',\n",
    "                                              'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15',\n",
    "                                              'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20']])\n",
    "\n",
    "df_select = pd.DataFrame(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélations\n",
    "Bien que les méthodes que nous allons implémenter dans ce projet gérent plutôt bien les variables corrélées, on peut afficher une matrice de corrélation pour mieux comprendre nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8, 8))\n",
    "#ax = plt.gca()\n",
    "plt.matshow(df_select.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(df.shape[1]), df.columns, fontsize=14)\n",
    "#ax.tick_params(axis=\"x\", bottom=True)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Matrice de corrélation', fontsize=16, y=-0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux CNN\n",
    "L'utilisation de réseaux de neurones convolutionnels représente la meilleure approche lorsqu'il s'agit de faire de la classification d'images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des images\n",
    "Dans la cellule ci-dessous, nous assoçions tous les fichiers audio à leur spectrogramme.\\\n",
    "Pour cela nous devons itérer sur ces fichiers, ploter leurs spectrogrammes respectifs et sauvegarder l'image dans un dossier que nous avons créé au préalable.\n",
    "\n",
    "Tous les spectrogrammes seront ensuite accessibles depuis le dictionnaire audio_files :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "audio_files = {}\n",
    "\n",
    "for g in genres:\n",
    "  audio_files[g] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itérer sur les fichiers audio dans le répertoire pour le genre actuel\n",
    "for g in genres:\n",
    "  for audio in os.listdir(f'./Data/genres_original/{g}'):\n",
    "    audio_files[g].append(librosa.core.load(f'./Data/genres_original/{g}/{audio}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "On peut maintenant stocker toutes les images sous forme de matrices numpy dans un vecteur X :\n",
    "\n",
    "Nous allons maintenant mettre en ordre nos données d'entraînement.\\\n",
    "On doit avoir un array que nous appelerons X (pour ne pas confondre avec le précédent X), constitué de toutes les matrices des images, ainsi qu'un array appelé y_cnn avec tous les labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_specs = []\n",
    "\n",
    "for g in genres:\n",
    "\n",
    "  for audio in audio_files[g]:\n",
    "\n",
    "    y = audio[0]\n",
    "    sr = audio[1]\n",
    "\n",
    "    spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "    spect = librosa.power_to_db(spect, ref=np.max)\n",
    "        \n",
    "# On modifie la taille des images 128 x 660 en gardant les paramètres proposés dans l'article initial\n",
    "    if spect.shape[1] != 660:\n",
    "      spect.resize(128,660, refcheck=False)\n",
    "\n",
    "    mel_specs.append(spect)\n",
    "        \n",
    "X = np.array(mel_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant créer le vecteur des labels y_cnn, pour l'instant nos images sont dans l'ordre de la construction de y_cnn est triviale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cnn = []\n",
    "\n",
    "for i in range(len(genres)):\n",
    "  y_cnn +=100*[i] # On a 100 images pour chaque genre\n",
    "y_cnn = np.array(y_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir utiliser y_cnn il faut la transformer en variables catégoriques par un encodage One-Hot, scikit-learn permet de le faire très facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On split ensuite nos données en données de test et données d'entraînements :\n",
    "y_cnn = to_categorical(y_cnn)\n",
    "x_cnn_train, x_cnn_test, y_cnn_train, y_cnn_test = train_test_split(X, y_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On pense à normaliser les données, c'est important pour l'analyse d'images :\n",
    "x_cnn_train /= -80\n",
    "x_cnn_test /= -80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A présent on met en forme nos données pour l'entraînement :\n",
    "x_cnn_train = x_cnn_train.reshape(x_cnn_train.shape[0], 128, 660, 1)\n",
    "x_cnn_test = x_cnn_test.reshape(x_cnn_test.shape[0], 128, 660, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade nous avons bien deux arrays X et y_cnn.\\\n",
    "X contient toutes les matrices représentants les images des spectrogrammes et y_cnn contient les labels pour chaque image.\n",
    "\n",
    "Vérifions les tailles de x_cnn et y_cnn pour être sur qu'il n'y a pas de problème :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_cnn_train.shape)\n",
    "print(y_cnn_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(128,660,1)))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de lancer l'entraînement du réseau, voyons un résumé des couches que l'on a :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compile le modèle en selectionnant la categorical crossentropy comme perte avec l'optimiseur ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut à présent commencer l'entraînement.\\\n",
    "Nous le faisons sur Google Colab pour pouvoir profiter d'un traitement parallèle sur GPU, ce qui représente un gain de temps considérable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_cnn_train,\n",
    "                    y_cnn_train,\n",
    "                    epochs=25,\n",
    "                    validation_data = (x_cnn_test,y_cnn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regader l'évolution des erreurs et de la loss au fur et à mesure des époques d'entraînements du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curve = history.history[\"loss\"]\n",
    "acc_curve = history.history[\"accuracy\"]\n",
    "\n",
    "loss_val_curve = history.history[\"val_loss\"]\n",
    "acc_val_curve = history.history[\"val_accuracy\"]\n",
    "\n",
    "plt.plot(loss_curve, label=\"Train\")\n",
    "plt.plot(loss_val_curve, label=\"Val\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_curve, label=\"Train\")\n",
    "plt.plot(acc_val_curve, label=\"Val\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilan des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "predictions = model.predict(x_cnn_test)\n",
    "mat = confusion_matrix(np.argmax(y_cnn_test, 1), np.argmax(predictions, 1))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=genres, yticklabels=genres)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('BadBoyModel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie prédiction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./BadBoyModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = './Data/genres_original/rock/rock.00054.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_file, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram of the audio file\n",
    "spectrogram = np.abs(librosa.stft(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the spectrogram to match the input shape of the model\n",
    "input_shape = (128, 660)\n",
    "spectrogram = resize(spectrogram, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the spectrogram to match the expected input shape of the model\n",
    "spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
    "spectrogram = np.expand_dims(spectrogram, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voici la liste des genres musicaux représentés dans la notre base de données, on en compte 10.\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the preprocessed audio\n",
    "predictions = model.predict(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class index\n",
    "predicted_class_index = np.argmax(predictions[0])\n",
    "\n",
    "# Get the name of the predicted class\n",
    "predicted_class_name = genres[predicted_class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class\n",
    "predicted_class = np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the name of the predicted class\n",
    "print(f'The predicted class is: {predicted_class_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AU final les prédictions sont assez mauvaises, car le modèle a été entrapiné sur les fichiers audio et non sur leurs images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement de méthode :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
